# -*- coding: utf-8 -*-
"""Copy of CIS545 project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yqXAEqhM6FIzDQhCRHSorq_gCGr-N1Q9

# **CIS545 Project**

# **0. Set up libraries and import data**
"""

!pip install pandasql

import json
import glob
import pandas as pd
import pandasql as ps #SQL on Pandas Dataframe
import numpy as np
import sqlite3
import datetime as dt
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly as py
import plotly.graph_objs as go
from sklearn.model_selection import train_test_split

"""**Step 1**:

Read csv files.

**Note**: import all the Kaggle files of the European Soccer Database to "/content/".

"""

from google.colab import drive
drive.mount('/content/drive')

! pip install kaggle

! mkdir ~/.kaggle

!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/

!kaggle datasets download -d hugomathien/soccer

!unzip /content/soccer.zip

con = sqlite3.connect("database.sqlite")
# country_df = pd.read_sql_query("SELECT * from country", con)
# league_df = pd.read_sql_query("SELECT * from league", con)
# match_df = pd.read_sql_query("SELECT * from match", con)
player_df = pd.read_sql_query("SELECT * from player", con)
player_attributes_df = pd.read_sql_query("SELECT * from player_attributes", con)
team_df = pd.read_sql_query("SELECT * from team", con)
team_attributes_df = pd.read_sql_query("SELECT * from team_attributes", con)

con.close()

"""To Do List: 
* Investigate data import  
* Link data between files
  *  Get team linked to match to player to get player team by year
* remove columns from tables that we do not want or need
* convert column data type to int/float
*   Determine list of players who appear in the first and last years of the data, as these players will be our primary cohort for testing?
* https://www.kaggle.com/code/manasaudupa/european-soccer-data-analysis-using-sql/notebook

# **1. Clean & Stage the Data**

To work with the data, we would require to clean the data i.e drop null values, remove columns that are not needed, drop duplicates, etc.

## **1.1 Drop null values**
"""

player_cleaned_df = player_df.dropna()
player_attributes_cleaned_df = player_attributes_df.dropna()
team_cleaned_df = team_df.dropna()
team_attributes_cleaned_df = team_attributes_df.dropna()

"""## **1.2 Drop Duplicates**"""

player_cleaned_df = player_cleaned_df.drop_duplicates()
player_attributes_cleaned_df = player_attributes_cleaned_df.drop_duplicates()
team_cleaned_df = team_cleaned_df.drop_duplicates()
team_attributes_cleaned_df = team_attributes_cleaned_df.drop_duplicates()
team_attributes_cleaned_df.head()

"""Let's check the datatypes for our player table



"""

player_attributes_cleaned_df.dtypes

player_cleaned_df.dtypes

"""## **1.3 Join the tables**

1.   Join player attributes and player table data using player_api_id
2.   Remove unwanted columns and rename the columns after joining for cleaner table data
"""

players_and_skills_df = player_attributes_cleaned_df.merge(player_cleaned_df, on="player_fifa_api_id")
players_and_skills_df = players_and_skills_df.drop(columns=['player_api_id_x', 'id_y', 'preferred_foot'])
players_and_skills_df = players_and_skills_df.rename(columns={"player_api_id_y": "player_api_id", "id_x" : "id"})
players_and_skills_df.head()

"""## **1.4 Calculate the age of the players**"""

players_and_skills_df['age'] = players_and_skills_df.apply(lambda x: pd.to_datetime(x['date']).year - pd.to_datetime(x['birthday']).year, axis=1)
players_and_skills_df.head()

"""We can also observe that our table has some categorical values that we will need to convert to integer/float to run our ML model. """

def assign(x):
  if x == "low":
    x = 3
    return x 
  elif x == "medium":
    x = 6
    return x
  elif x == "high":
    x = 9
    return x
  else:
    return 0

players_and_skills_df['defensive_work_rate'] = players_and_skills_df['defensive_work_rate'].apply(lambda x: assign(x))
players_and_skills_df['attacking_work_rate'] = players_and_skills_df['attacking_work_rate'].apply(lambda x: assign(x))
players_and_skills_df.head()

"""## **1.5 Visualize data**

Let's visualize what our distribution of players looks like by year:
"""

player_year_query = """
WITH player_years AS (
    SELECT player_fifa_api_id,
           (strftime('%Y-%m',date)) AS year_month
    FROM players_and_skills_df
    
    GROUP BY player_fifa_api_id, (strftime('%Y-%m',date))
)

SELECT year_month, COUNT(player_fifa_api_id) AS player_count
FROM player_years
GROUP BY year_month
"""
sql_player_year_df = ps.sqldf(player_year_query, locals())

def addlabels(x,y):
    for i in range(len(x)):
        plt.text(i, y[i], y[i], ha = 'center')

plt.subplots(figsize=(18,5))
plt.bar(sql_player_year_df['year_month'], sql_player_year_df['player_count'])
plt.title("Distribution of Player Count by Year-Month",fontsize=18)
plt.xlabel('Year', fontsize=14)
plt.ylabel('# of Distinct Players', fontsize=14)
plt.xticks(rotation=90)
addlabels(sql_player_year_df['year_month'], sql_player_year_df['player_count'])
plt.show()

#https://www.geeksforgeeks.org/adding-value-labels-on-a-matplotlib-bar-chart/

"""We will limit our data to just full soccer seasons, which begin in August. Therefore we can remove the data in 2007-02. """

#Remove the players that get added in 2007-02
players_and_skills_df = players_and_skills_df[players_and_skills_df['date'] >= '2007-08-01']

"""# **2. Analysis**

## **2.1 Find correlation between Player skills**
"""

player_skills_heatmap = players_and_skills_df.drop(columns=['id', 'player_fifa_api_id', 'player_api_id'])

mask = np.zeros_like(player_skills_heatmap.corr(), dtype=np.bool)
mask[np.triu_indices_from(mask)] = True

plt.figure(figsize=(14, 10))
sns.heatmap(player_skills_heatmap.corr(), mask=mask, cmap="RdBu")

"""## **2.2 Understanding key aspects of player data**

We want to understand how many years players we have in the data
"""

player_avg_year_query = """
WITH player_years AS (
    SELECT player_fifa_api_id,
           (strftime('%Y',date)) AS year
    FROM players_and_skills_df
    GROUP BY player_fifa_api_id, (strftime('%Y',date))
),
player_avg_years AS (
  SELECT player_fifa_api_id,
          COUNT(DISTINCT year) AS data_years
  FROM player_years
  GROUP BY player_fifa_api_id 
)

SELECT data_years, COUNT(player_fifa_api_id) AS player_count
FROM player_avg_years
GROUP BY data_years
"""
sql_player_avg_year_df = ps.sqldf(player_avg_year_query, locals())

def addlabels(x,y):
    for i in range(len(x)):
        plt.text(i, y[i], y[i], ha = 'center')

plt.subplots(figsize=(18,8))
plt.bar(sql_player_avg_year_df['data_years'], sql_player_avg_year_df['player_count'])
plt.title("Distribution of Player Count by Years in Data",fontsize=18)
plt.xlabel('# of Years', fontsize=14)
plt.ylabel('# of Players', fontsize=14)
plt.xticks(np.arange(1,len(sql_player_avg_year_df['data_years'])+1, step=1))
addlabels(sql_player_avg_year_df['data_years'], sql_player_avg_year_df['player_count'])
plt.show()

"""It appears that we have over 2000 players with a full 10 years in the data. With this cohort of players with the most data, we can see when they "peaked" by finding the last date they had their highest 'overall_rating' score. 

For a given player, we want to answer the following question: 
*   Over the next period/season, will the player's overall rating be in RISE or DECLINE phase?

We'll develop machine learning models to answer this question. Our models will be a "screening" method for scouters 

"""

#First limit to players with 10 years of results
player_yrs_10_query = """
WITH player_years AS (
    SELECT player_fifa_api_id,
           (strftime('%Y',date)) AS year
    FROM players_and_skills_df
    GROUP BY player_fifa_api_id, (strftime('%Y',date))
),
player_avg_years AS (
  SELECT player_fifa_api_id,
          COUNT(DISTINCT year) AS data_years
  FROM player_years
  GROUP BY player_fifa_api_id 
)

SELECT player_fifa_api_id
FROM player_avg_years
WHERE data_years >= 10
GROUP BY player_fifa_api_id
"""
sql_player_yrs_10_df = ps.sqldf(player_yrs_10_query, locals())

#Join table to master table and limit data to just these players
player_ratings_limited_query = """
WITH player_rating_age AS (
    SELECT player_fifa_api_id, overall_rating, age
    FROM players_and_skills_df
    GROUP BY player_fifa_api_id, overall_rating, age
)

SELECT X.player_fifa_api_id, X.overall_rating, X.age
FROM player_rating_age X
    JOIN sql_player_yrs_10_df Y on X.player_fifa_api_id = Y.player_fifa_api_id
GROUP BY X.player_fifa_api_id, X.overall_rating, X.age
"""
sql_player_ratings_limited_df = ps.sqldf(player_ratings_limited_query, locals())

#Determine the max overall rating for these players. If multiple, then select latest date
player_ratings_peak_query = """
WITH player_rating_max AS (
    SELECT player_fifa_api_id, max(overall_rating) AS max_rating
    FROM sql_player_ratings_limited_df X
    GROUP BY player_fifa_api_id
),
player_rating_age_max AS (
    SELECT X.player_fifa_api_id, MAX(X.age) as max_age
    FROM sql_player_ratings_limited_df X
      JOIN player_rating_max Y on X.player_fifa_api_id = Y.player_fifa_api_id AND X.overall_rating = Y.max_rating
    GROUP BY X.player_fifa_api_id
)

SELECT X.player_fifa_api_id, 
       X.overall_rating, 
       X.age,
       CASE
          WHEN X.age < Y.max_age THEN 'RISE'
          WHEN X.age = Y.max_age AND X.overall_rating < Z.max_rating THEN 'RISE'
          WHEN X.age = Y.max_age AND X.overall_rating = Z.max_rating THEN 'PEAK'
          ELSE 'DECLINE'
      END AS peak_flg
FROM sql_player_ratings_limited_df X
    LEFT JOIN player_rating_age_max Y 
      on X.player_fifa_api_id = Y.player_fifa_api_id
    LEFT JOIN player_rating_max Z 
      on X.player_fifa_api_id = Z.player_fifa_api_id 
GROUP BY X.player_fifa_api_id, 
       X.overall_rating, 
       X.age,
       CASE
          WHEN X.age < Y.max_age THEN 'RISE'
          WHEN X.age = Y.max_age AND X.overall_rating < Z.max_rating THEN 'RISE'
          WHEN X.age = Y.max_age AND X.overall_rating = Z.max_rating THEN 'PEAK'
          ELSE 'DECLINE'
      END
"""
sql_player_ratings_peak_df = ps.sqldf(player_ratings_peak_query, locals())

"""Doing a check to see what the data looks like to ensure that we are properly capturing the rise, peak, and decline"""

sql_player_ratings_peak_df[(sql_player_ratings_peak_df['player_fifa_api_id'] == 684)].sort_values(by = 'age').head(100)
#sql_player_ratings_peak_df[(sql_player_ratings_peak_df['age'] == 18) & (sql_player_ratings_peak_df['peak_flg'] == 'PEAK')] #684 11983 158718 189484

#Plot the distribution of the average age they reach peak.
player_peak_query = """
SELECT COUNT(player_fifa_api_id) AS player_count, 
       overall_rating, 
       age
FROM sql_player_ratings_peak_df  
WHERE peak_flg = 'PEAK'
GROUP BY overall_rating, 
       age
"""
players_peak_df = ps.sqldf(player_peak_query, locals())

def addlabels(x,y):
    for i in range(len(x)):
        plt.text(i, y[i], y[i], ha = 'center')

plt.subplots(figsize=(18,8))
plt.bar(players_peak_df['age'], players_peak_df['player_count'])
plt.title("Distribution of Player Rating Peak by Age",fontsize=18)
plt.xlabel('Age', fontsize=14)
plt.ylabel('# of Players', fontsize=14)
plt.xticks(np.arange(min(players_peak_df['age']),max(players_peak_df['age'])+1, step=1))
plt.show()

#From peak, plot the average trajectory of score decline 
#For consistency, take the highest number in each year, so we can compare declines accurately
player_decline_query = """
WITH decline_years AS (
  SELECT player_fifa_api_id,
        overall_rating, 
        age
  FROM sql_player_ratings_peak_df
  WHERE peak_flg = 'DECLINE'
  GROUP BY player_fifa_api_id,
        overall_rating, 
        age
)

SELECT X.player_fifa_api_id,
      MAX(X.overall_rating) AS rating_year,
      X.age
  FROM players_and_skills_df X
      JOIN decline_years Y ON X.player_fifa_api_id = Y.player_fifa_api_id AND X.overall_rating = Y.overall_rating AND X.age = Y.age
  GROUP BY X.player_fifa_api_id,
      X.age
"""
player_decline_df = ps.sqldf(player_decline_query, locals())

#Find the average score by years after peak 
player_decline_df['rank'] = player_decline_df.groupby(by = 'player_fifa_api_id')['age'].rank(method = 'first')
player_avg_decline_df = player_decline_df.groupby(by = 'rank')['rating_year'].mean().reset_index(name = 'avg_rating')

#Plot the resulting line
def addlabels(x,y):
    for i in range(len(x)):
        plt.text(i+1, y[i], y[i], ha = 'center')

plt.subplots(figsize=(18,8))
plt.plot(player_avg_decline_df['rank'], player_avg_decline_df['avg_rating'])
plt.title("Average Decline Across Players",fontsize=18)
plt.xlabel('Years Since Peak', fontsize=14)
plt.ylabel('Average Rating', fontsize=14)
plt.xticks(np.arange(min(player_avg_decline_df['rank']),max(player_avg_decline_df['rank'])+1, step=1))
addlabels(player_avg_decline_df['rank'], round(player_avg_decline_df['avg_rating'],1))
plt.show()

"""## **2.3 Finalizing our dataset for classification analyses**

After performing the above analyses, we determined that we will need at least three years of data to have a baseline understanding of a players rise, peak, and decline. We will limit our dataset to players with three years and then proceed to add a column with the three classifications of a players trajectory

We understand there a limitations to this approach, since we are no viewing a players full rise and decline. However, we believe that defining this trajectory will be sufficiently useful for classification and testing our predictions. 

For the below additional columns, we will limit to 'Rise' and 'Decline' in order make the column entries binary. We understand a player's peak to be their final rating at 'Rise'.  
"""

#First limit to players with 3+ years of results
player_yrs_3_query = """
WITH player_years AS (
    SELECT player_fifa_api_id,
           (strftime('%Y',date)) AS year
    FROM players_and_skills_df
    GROUP BY player_fifa_api_id, (strftime('%Y',date))
),
player_avg_years AS (
  SELECT player_fifa_api_id,
          COUNT(DISTINCT year) AS data_years
  FROM player_years
  GROUP BY player_fifa_api_id 
)

SELECT player_fifa_api_id
FROM player_avg_years
WHERE data_years >= 3
GROUP BY player_fifa_api_id
"""
player_yrs_3_df = ps.sqldf(player_yrs_3_query, locals())

#Join table to master table and limit data to just these players
player_ratings_3_query = """
WITH player_rating_age AS (
    SELECT player_fifa_api_id, overall_rating, age
    FROM players_and_skills_df
    GROUP BY player_fifa_api_id, overall_rating, age
)

SELECT X.player_fifa_api_id, X.overall_rating, X.age
FROM player_rating_age X
    JOIN player_yrs_3_df Y on X.player_fifa_api_id = Y.player_fifa_api_id
GROUP BY X.player_fifa_api_id, X.overall_rating, X.age
"""
sql_player_ratings_3_df = ps.sqldf(player_ratings_3_query, locals())

#Determine the max overall rating for these players. If multiple, then select latest date
player_ratings_peak_3_query = """
WITH player_rating_max AS (
    SELECT player_fifa_api_id, max(overall_rating) AS max_rating
    FROM sql_player_ratings_3_df X
    GROUP BY player_fifa_api_id
),
player_rating_age_max AS (
    SELECT X.player_fifa_api_id, MAX(X.age) as max_age
    FROM sql_player_ratings_3_df X
      JOIN player_rating_max Y on X.player_fifa_api_id = Y.player_fifa_api_id AND X.overall_rating = Y.max_rating
    GROUP BY X.player_fifa_api_id
)

SELECT X.player_fifa_api_id, 
       X.overall_rating, 
       X.age,
       CASE
          WHEN X.age < Y.max_age THEN 'RISE'
          WHEN X.age = Y.max_age AND X.overall_rating < Z.max_rating THEN 'RISE'
          WHEN X.age = Y.max_age AND X.overall_rating = Z.max_rating THEN 'RISE'
          ELSE 'DECLINE'
      END AS peak_flg
FROM sql_player_ratings_3_df X
    LEFT JOIN player_rating_age_max Y 
      on X.player_fifa_api_id = Y.player_fifa_api_id
    LEFT JOIN player_rating_max Z 
      on X.player_fifa_api_id = Z.player_fifa_api_id 
GROUP BY X.player_fifa_api_id, 
       X.overall_rating, 
       X.age,
       CASE
          WHEN X.age < Y.max_age THEN 'RISE'
          WHEN X.age = Y.max_age AND X.overall_rating < Z.max_rating THEN 'RISE'
          WHEN X.age = Y.max_age AND X.overall_rating = Z.max_rating THEN 'RISE'
          ELSE 'DECLINE'
      END
"""
sql_player_ratings_peak_3_df = ps.sqldf(player_ratings_peak_3_query, locals())

#Merge datasets to make a master dataset for usage below 
TEMP_players_and_skills_df = players_and_skills_df.merge(sql_player_ratings_peak_3_df, on= ["player_fifa_api_id", "overall_rating", "age"])
TEMP_players_and_skills_df['rank'] = TEMP_players_and_skills_df.groupby(by = 'player_fifa_api_id')['date'].rank(method = 'first')

#Merge again with next row to get the future peak flag
MASTER_players_and_skills_query = """
SELECT X.*,
      Y.peak_flg AS peak_flg_2

FROM TEMP_players_and_skills_df X
  LEFT JOIN TEMP_players_and_skills_df Y ON X.player_fifa_api_id = Y.player_fifa_api_id AND X.rank = Y.rank - 1
"""
MASTER_players_and_skills_df = ps.sqldf(MASTER_players_and_skills_query, locals())

MASTER_players_and_skills_df = MASTER_players_and_skills_df.drop(columns = ['rank'])

MASTER_players_and_skills_df[(MASTER_players_and_skills_df['player_fifa_api_id'] == 11983)].sort_values(by = ['player_fifa_api_id','date']).tail(10)

MASTER_players_and_skills_df.describe()

"""## **2.4 Selecting a subset of players to further analyze**
We'll select a group of players and then visualize the evolution of their overall rating over their last 10 periods of data (last 10 data entries for each player).

After we build our classification models, we'll use our best model to make the following prediction for each player: **Will a player's overall rating be in DECLINE phase over the next period?** For some of them who are in their **RISE** phase, the question is even more interesting.
"""

#Query to get historical data
historical_data_query = """
SELECT player_name, overall_rating, date
FROM MASTER_players_and_skills_df
WHERE player_name IN ('Lionel Messi', 'Santi Cazorla', 'Sergio Garcia','Gerard Pique', 'Cristiano Ronaldo', 'Edinson Cavani', 'Eden Hazard', 'Virgil van Dijk', 'Andres Iniesta')
"""
historical_rating_df = ps.sqldf(historical_data_query, locals())
historical_rating_df

#Assign row numbers (periods of data) to each player and keep only the last 10 periods
historical_rating_df.loc[:, ('period_backwards')] = historical_rating_df.groupby('player_name')['date'].rank(method='first', ascending=False)
historical_rating_df = historical_rating_df[historical_rating_df['period_backwards']<=10]
historical_rating_df.loc[:, ('period')] = historical_rating_df['period_backwards'].apply(lambda x: 11 - x)
historical_rating_df.drop(columns=['period_backwards'], inplace=True)

historical_rating_df.head(30)

players_plot = sns.lineplot(data=historical_rating_df, x="period", y="overall_rating", hue="player_name")
plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
plt.yticks([x for x in range(70,100) if x%2 == 0]);

"""# **3. Classification Models**

## **3.1 Modeling**

We're going to create two dataframes to separate goalkeeping skills from player skills.
"""

player_skills = MASTER_players_and_skills_df.drop(columns=['gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning', 'gk_reflexes'])
goalkeeper_skills = MASTER_players_and_skills_df[['gk_diving', 'gk_handling', 'gk_kicking', 'gk_positioning', 'gk_reflexes']]

"""After separating player's and goalkeeper's skills, we're going to separate our final player dataset that we will test on after training or models. 

The rows we're going to separate have `peak_flg_2` column as 'None', which basically means that there is no data for that particular player after this point in time. 

We will resolve this issue via our ML models by training our it on historical values and tehn predicting their future performance.   
"""

final_player_df = player_skills[player_skills['peak_flg_2'].isnull()]
final_player_df

"""Let's separate all rows that have `peak_flg_2` as None value. This is because `peak_flg_2` column predicts rise/decline based on next(future) datapoint's attribute and since these rows are last in the dataset, they can't be predicted on. 

We will be predicting the future performace on those rows once our model is created. 


"""

player_skills = player_skills[(player_skills['peak_flg_2'] == 'RISE') | (player_skills['peak_flg_2'] == 'DECLINE')]
player_skills

"""### 3.1.1 Creating the classification labels

To run machine learning models all our column values need to be integer or float. Since our `peak flag` column is a categorical value, we will change it to a binary value. If the `peak flag` is `RISE`, we will change it to 1 and if it is `DECLINE`, we will change it to 0.
"""

player_skills['peak_boolean'] = player_skills['peak_flg_2'].apply(lambda x: 0 if x == 'DECLINE' else 1)

"""Since we need to start classifying data into features and labels to train and test, we create two dataframes for the same."""

player_skills_features = player_skills.drop(columns=['id', 'player_fifa_api_id', 'player_api_id', 'date', 'potential', 'player_name', 'date', 'birthday', 'height', 'weight', 'peak_flg', 'peak_flg_2', 'peak_boolean'])
player_rating_label = player_skills['peak_boolean']
player_rating_label = pd.Series(player_rating_label)

player_skills_features

player_rating_label

"""### 3.1.2 Split data into test and train """

seed = 42
x_train, x_test, y_train, y_test = train_test_split(player_skills_features, player_rating_label, test_size=0.2, random_state=seed)

"""### 3.1.3 Scaling the data """

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
pca_scaled = scaler.fit(x_train).transform(x_train)

"""##**3.2 Logistic Regression Classifier**"""

import numpy as np
from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# intermediate step to address scale-invariance
scaler = StandardScaler()
x_train_sc = scaler.fit_transform(x_train)
x_test_sc = scaler.transform(x_test)

# train the logistic regression model
lr = LogisticRegression(max_iter=100, solver='liblinear')
lr.fit(x_train_sc,y_train)

train_pred_lr = lr.predict(x_train_sc)
test_pred_lr = lr.predict(x_test_sc)

from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_true=y_train, y_pred=train_pred_lr)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix for Logistic Regression model (Training dataset)', fontsize=18)
plt.show()

print("Metrics for Training dataset:\n")
print("precision: "+str(metrics.precision_score(y_train, train_pred_lr)))
print("accuracy: "+str(metrics.accuracy_score(y_train, train_pred_lr)))
print("recall: "+str(metrics.recall_score(y_train, train_pred_lr)))

conf_matrix = confusion_matrix(y_true=y_test, y_pred=test_pred_lr)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix for Logistic Regression model (Test dataset)', fontsize=18)
plt.show()

print("Metrics for Test dataset:\n")
print("precision: "+str(metrics.precision_score(y_test, test_pred_lr)))
print("accuracy: "+str(metrics.accuracy_score(y_test, test_pred_lr)))
print("recall: "+str(metrics.recall_score(y_test, test_pred_lr)))

"""##**3.3 SVM's LinearSVC classifier**"""

from sklearn import svm

# train the Linear SVC model
svmmodel = svm.LinearSVC(random_state=0, tol=1e-5)
svmmodel.fit(x_train_sc,y_train)

train_pred_svm = svmmodel.predict(x_train_sc)
test_pred_svm = svmmodel.predict(x_test_sc)

conf_matrix = confusion_matrix(y_true=y_train, y_pred=train_pred_svm)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix for LinearSVC (Training dataset)', fontsize=18)
plt.show()

print("Metrics for Training dataset:\n")
print("precision: "+str(metrics.precision_score(y_train, train_pred_svm)))
print("accuracy: "+str(metrics.accuracy_score(y_train, train_pred_svm)))
print("recall: "+str(metrics.recall_score(y_train, train_pred_svm)))

conf_matrix = confusion_matrix(y_true=y_test, y_pred=test_pred_svm)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix for LinearSVC (Test dataset)', fontsize=18)
plt.show()

print("Metrics for Test dataset:\n")
print("precision: "+str(metrics.precision_score(y_test, test_pred_svm)))
print("accuracy: "+str(metrics.accuracy_score(y_test, test_pred_svm)))
print("recall: "+str(metrics.recall_score(y_test, test_pred_svm)))

"""##**3.4 Random Forest classifier**"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(max_depth=28,random_state=42)
rf.fit(x_train, y_train)

x_train

train_pred_rf = rf.predict(x_train)
test_pred_rf = rf.predict(x_test)

conf_matrix = confusion_matrix(y_true=y_train, y_pred=train_pred_rf)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix for Random Forest model (Training dataset)', fontsize=18)
plt.show()

print("Metrics for Training dataset:\n")
print("precision: "+str(metrics.precision_score(y_train, train_pred_rf)))
print("accuracy: "+str(metrics.accuracy_score(y_train, train_pred_rf)))
print("recall: "+str(metrics.recall_score(y_train, train_pred_rf)))

conf_matrix = confusion_matrix(y_true=y_test, y_pred=test_pred_rf)
fig, ax = plt.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix for Random Forest model (Test dataset)', fontsize=18)
plt.show()

print("Metrics for Test dataset:\n")
print("precision: "+str(metrics.precision_score(y_test, test_pred_rf)))
print("accuracy: "+str(metrics.accuracy_score(y_test, test_pred_rf)))
print("recall: "+str(metrics.recall_score(y_test, test_pred_rf)))

"""Comparing the models, we can see that Random Forest performs the best and give us an accuracy of 94%. 

Now, We will use our Random Forest model to predict on all player's for the latest season.

## 3.5 Model Performance Review
"""

xlr = [1,2,3]
xsvm  = [1,2,3]
xrf  = [1,2,3]


ylr = [(metrics.precision_score(y_test, test_pred_lr)),(metrics.accuracy_score(y_test, test_pred_lr)),(metrics.recall_score(y_test, test_pred_lr))]
ysvm = [(metrics.precision_score(y_test, test_pred_svm)),(metrics.accuracy_score(y_test, test_pred_svm)),(metrics.recall_score(y_test, test_pred_svm))]
yrf = [(metrics.precision_score(y_test, test_pred_rf)),(metrics.accuracy_score(y_test, test_pred_rf)),(metrics.recall_score(y_test, test_pred_rf))]

xticks=['precision','accuracy','recall']



plt.plot(xlr,ylr,'bo', label = 'LogReg', marker = 's')
plt.plot(xsvm,ysvm,'go', label = 'SVM', marker = 'o')
plt.plot(xrf,yrf,'ro', label = 'Random Forest', marker = '^')
plt.xticks(xlr,xticks )
plt.title("Comparison of Model Metrics",fontsize=18)
plt.xlabel('Measure', fontsize=14)
plt.ylabel('Percentage', fontsize=14)
plt.legend(bbox_to_anchor=(1.02, 1),loc = 'upper left')
plt.show()

"""# **4. Predictions on Final dataset**

Now that we are done fine-tuning our model, it is ready to be used on our main and final dataset.
To recall, our dataset had values from 2007-2016. We have trained our dataset to predict on player's future performance. This section will predict on all the player's (mentioned in the dataset) future performance in the next period/season of data. 

Previously we had separated out a final_players_df which had `peak_flg_2` column 'None'. Now we will fill this solumn based on our model.
"""

# final_player_df
final_player_test_df = final_player_df.drop(columns=['id', 'player_fifa_api_id', 'player_api_id', 'date', 'potential', 'player_name', 'date', 'birthday', 'height', 'weight', 'peak_flg', 'peak_flg_2'])
final_player_test_df

rf.predict(final_player_test_df)

"""Let's convert the predictions into categorical values for better undertsanding. Like previously done, we change 1 to `RISE` and 0 to `DECLINE`."""

final_player_df.loc[:, ('predictions')] = rf.predict(final_player_test_df)
final_player_df.loc[:, ('peak_flg_2')] = final_player_df['predictions'].apply(lambda x: 'RISE' if x == 1 else 'DECLINE')
final_player_df

"""Re-ordering column names so it's clearer to understand"""

final_player_df = final_player_df[['id', 'player_fifa_api_id', 'player_name', 'age', 'date', 'peak_flg', 'peak_flg_2', 'overall_rating', 'potential',
       'attacking_work_rate', 'defensive_work_rate', 'crossing', 'finishing',
       'heading_accuracy', 'short_passing', 'volleys', 'dribbling', 'curve',
       'free_kick_accuracy', 'long_passing', 'ball_control', 'acceleration',
       'sprint_speed', 'agility', 'reactions', 'balance', 'shot_power',
       'jumping', 'stamina', 'strength', 'long_shots', 'aggression',
       'interceptions', 'positioning', 'vision', 'penalties', 'marking',
       'standing_tackle', 'sliding_tackle', 'player_api_id', 
       'birthday', 'height', 'weight', 'predictions']]

"""We will now see predictions for players that we had earlier picked out in section 2.4.

`peak_flg` and `peak_flg_2` respectively represent a player's last recorded performance and predicted future performance (in the next period). 

For example, **Eden Hazard** was on the declining curve (see plot on section 2.4) but as of January 2016, has started to rise back. This makes a lot of sense since his Potential Rating (given by FIFA) is greater than his current Rating.

Another notable case is **Virgil Van Dijk**, whose Potential Rating is greater than his current Rating, and whose skills started to raise (see plot in section 2.4). We can see that our model predicts that he will continue in the RISE stage over the next period.

Finally, there are some cases such as **Santi Cazorla** and **Sergio Garcia**. They are currently in their RISE stage (as we can see in section 2.4), but our model predicts that their curve will change their slopes to DECLINE. 

*Thus, a scout studying the acquision of those players at that time should be cautious about the signing!*
"""

final_player_df[final_player_df['player_name'].isin(['Lionel Messi', 'Santi Cazorla', 'Sergio Garcia','Gerard Pique', 'Cristiano Ronaldo', 'Edinson Cavani', 'Eden Hazard', 'Virgil van Dijk', 'Andres Iniesta'])]